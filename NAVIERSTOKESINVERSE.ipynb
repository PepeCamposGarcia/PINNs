{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model definition\n"
      ],
      "metadata": {
        "id": "faqhBKapzN53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cWKzRA20TSM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PepeCamposGarcia/PINNs.git"
      ],
      "metadata": {
        "id": "kUrn3puZzR9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same fluid and physical phenomena as the direct case, but in this one, the aim of the model is to infer the parameter nu and rho that describe the fluid from the flow fields we know.\n",
        "$$\n",
        "    \\begin{cases}\n",
        "    \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} + v \\frac{\\partial u}{\\partial y} = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial x} + \\nu \\left( \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} \\right), \\\\\n",
        "     \\frac{\\partial v}{\\partial t} + u \\frac{\\partial v}{\\partial x} + v \\frac{\\partial v}{\\partial y}  = -\\frac{1}{\\rho}\\frac{\\partial p}{\\partial y} + \\nu \\left( \\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} \\right), \\\\\n",
        "     \\frac{\\partial u}{\\partial x} + \\frac{\\partial v}{\\partial y} = 0,\n",
        "    \\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "RqoIWTApzfpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalActivation(nn.Module):\n",
        "  def forward(self,x):\n",
        "    return torch.sin(2 * np.pi * x)"
      ],
      "metadata": {
        "id": "YccUIRzU0d3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arbitrary initialization of parameters.\n",
        "\n",
        "Real values are nu = 0.01 and rho = 1"
      ],
      "metadata": {
        "id": "C0whuZXW0BOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nu = 0.005\n",
        "rho = 0.1"
      ],
      "metadata": {
        "id": "5Bs69RwoAiin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NavierStokesModel(nn.Module):\n",
        "\n",
        "############################### NET ARCHITECTURE ###############################\n",
        "\n",
        "  def __init__(self, num_neurons, num_layers):\n",
        "\n",
        "    super(NavierStokesModel, self).__init__()\n",
        "    self.num_neurons = num_neurons\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Inclusion of nu and rho in model set of parameters\n",
        "\n",
        "    self.nu = torch.tensor([nu], dtype=torch.float32, requires_grad = True)\n",
        "    self.rho = torch.tensor([rho], dtype=torch.float32, requires_grad = True)\n",
        "\n",
        "    self.nu = nn.Parameter(self.nu)\n",
        "    self.rho = nn.Parameter(self.rho)\n",
        "\n",
        "    self.register_parameter('nu', self.nu)\n",
        "    self.register_parameter('rho', self.rho)\n",
        "\n",
        "\n",
        "### Imput layer, fully connected to the first hidden layer, sinusoidal activation\n",
        "### function for the dismiss of local minima\n",
        "\n",
        "    layer_list = [nn.Linear(3, self.num_neurons)]\n",
        "    layer_list.append(SinusoidalActivation())\n",
        "\n",
        "### Loop for the description of the hidden layers, fully connected layers\n",
        "### hiperbolic tangent activation function\n",
        "\n",
        "    for _ in range (self.num_layers - 2):\n",
        "      layer_list.append(nn.Linear(self.num_neurons, self.num_neurons))\n",
        "      layer_list.append(nn.Tanh())\n",
        "\n",
        "### Output layer, 3 outputs (u,v,p)\n",
        "\n",
        "    layer_list.append(nn.Linear(self.num_neurons, 3))\n",
        "    self.layers = nn.ModuleList(layer_list)\n",
        "\n",
        "\n",
        "\n",
        "################## FEED-FORWARD PROPAGATION, OUTPUTS ###########################\n",
        "\n",
        "  def forward(self, x, y, t):\n",
        "     \"\"\"\n",
        "        Params:\n",
        "        x - array of shape (N, 1), input x coordinates\n",
        "        y - array of shape (N, 1), input y coordinates\n",
        "        t - array of shape (N, 1), input time coordinate\n",
        "        Returns:\n",
        "        u - tensor of shape (N, 1), output x-velocity\n",
        "        v - tensor of shape (N, 1), output y-velocity\n",
        "        p - tensor of shape (N, 1), output pressure\n",
        "        f - x-momentum PDE evaluation of shape (N, 1)\n",
        "        g - y-momentum PDE evaluation of shape (N, 1)\n",
        "        h - continuity PDE evaluation of shape (N, 1)\n",
        "\n",
        "     \"\"\"\n",
        "     x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
        "     y = torch.tensor(y, dtype=torch.float32, requires_grad=True)\n",
        "     t = torch.tensor(t, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "     input_data = torch.hstack([x, y, t])\n",
        "     self.N = input_data.shape[0]\n",
        "\n",
        "     out = input_data\n",
        "\n",
        "     for layer in self.layers:\n",
        "      out = layer(out)\n",
        "\n",
        "     u, v, p = out[:, [0]], out[:, [1]], out[:, [2]] # (N, 1) each\n",
        "\n",
        "     u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "     u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "     u_y = torch.autograd.grad(u, y, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "     u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "     u_yy = torch.autograd.grad(u_y, y, grad_outputs=torch.ones_like(u_y), create_graph=True)[0]\n",
        "\n",
        "     v_t = torch.autograd.grad(v, t, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "     v_x = torch.autograd.grad(v, x, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "     v_y = torch.autograd.grad(v, y, grad_outputs=torch.ones_like(v), create_graph=True)[0]\n",
        "     v_xx = torch.autograd.grad(v_x, x, grad_outputs=torch.ones_like(v_x), create_graph=True)[0]\n",
        "     v_yy = torch.autograd.grad(v_y, y, grad_outputs=torch.ones_like(v_y), create_graph=True)[0]\n",
        "\n",
        "     p_x = torch.autograd.grad(p, x, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "     p_y = torch.autograd.grad(p, y, grad_outputs=torch.ones_like(p), create_graph=True)[0]\n",
        "\n",
        "        # Evaluate momentum PDE's\n",
        "\n",
        "     f = u_t + u * u_x + v * u_y + (1 / self.rho) * p_x - self.nu * (u_xx + u_yy) # (N, 1)\n",
        "     g = v_t + u * v_x + v * v_y + (1 / self.rho) * p_y - self.nu * (v_xx + v_yy) # (N, 1)\n",
        "\n",
        "        # Evaluate continuity PDE\n",
        "\n",
        "     h = u_x + v_y\n",
        "\n",
        "     return u, v, p, f, g, h"
      ],
      "metadata": {
        "id": "18dWhrTZ0e26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### LOSS FUNCTION DESIGN #####################################\n",
        "\n",
        "class NavierStokesLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.mse = nn.MSELoss()\n",
        "\n",
        "  def forward(self, u, u_net, v, v_net, p, p_net, f, g, h):\n",
        "     u = torch.tensor(u, dtype=torch.float32)\n",
        "     v = torch.tensor(v, dtype=torch.float32)\n",
        "     p = torch.tensor(p, dtype=torch.float32)\n",
        "\n",
        "\n",
        "     L_data = self.mse(u, u_net) + self.mse(v, v_net) + self.mse(p, p_net)\n",
        "     L_pde  = self.mse(f, torch.zeros_like(f)) + self.mse(g, torch.zeros_like(g)) + self.mse(h, torch.zeros_like(h))\n",
        "\n",
        "     return L_data + L_pde"
      ],
      "metadata": {
        "id": "FSBLpUm80fDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data loading"
      ],
      "metadata": {
        "id": "d_tEB8PJ0ckd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy"
      ],
      "metadata": {
        "id": "CphOO-oK0fMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loading ():\n",
        "\n",
        "  data = scipy.io.loadmat('/content/PINNs/Models_Data_Figures/Data/cylinder_wake.mat')\n",
        "  U_star = data['U_star']  # N x 2 x T   5000 x 2 x 200\n",
        "  P_star = data['p_star']  # N x T       5000 x 200\n",
        "  t_star = data['t']       # T x 1       200  x 1\n",
        "  X_star = data['X_star']  # N x 2       5000 x 2\n",
        "\n",
        "  N, T= X_star.shape[0], t_star.shape[0]\n",
        "\n",
        "### Dimension transformation into N x T\n",
        "\n",
        "  XX = np.tile(X_star[:, [0]], (1, T))\n",
        "  YY = np.tile(X_star[:, [1]], (1, T))\n",
        "  TT = np.tile(t_star, (1, N)).T\n",
        "  UU = U_star[:, 0, :]\n",
        "  VV = U_star[:, 1, :]\n",
        "  PP = P_star\n",
        "\n",
        "### Column vector transformation dim NT x 1\n",
        "\n",
        "  x_in = XX.flatten().reshape(-1, 1)  # NT x 1\n",
        "  y_in = YY.flatten().reshape(-1, 1)  # NT x 1\n",
        "  t_in = TT.flatten().reshape(-1, 1)  # NT x 1\n",
        "  u_in = UU.flatten().reshape(-1, 1)  # NT x 1\n",
        "  v_in = VV.flatten().reshape(-1, 1) # NT x 1\n",
        "  p_in = PP.flatten().reshape(-1, 1) # NT x 1\n",
        "\n",
        "  return x_in, y_in, t_in, u_in, v_in, p_in, (N, T)\n"
      ],
      "metadata": {
        "id": "7F4WKBLM_-Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_in, y_in, t_in, u_in, v_in, p_in, (N, T) = data_loading()"
      ],
      "metadata": {
        "id": "Y8RAuk5MAE5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "eEGWcIznAIxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boundary_indices(N, T):\n",
        "    \"\"\"\n",
        "    Returns a boolean mask marking the boundary condition for all timesteps\n",
        "    Params:\n",
        "    N - # of data samples (space locations) in problem\n",
        "    T - # of timesteps\n",
        "    Return:\n",
        "    nd-array of shape (N*T, )\n",
        "    \"\"\"\n",
        "    # Create grid for one timestep\n",
        "    # Data is a 100 x 50 matriz for each timestep, each timestep constains\n",
        "    # 296 boundary points out of the 5000 total, this function creates a boolean mask\n",
        "    # to identify them\n",
        "\n",
        "    grid_t_0 = np.zeros((50, 100))\n",
        "\n",
        "    # Set boundary to 1\n",
        "    grid_t_0[0, :] = 1\n",
        "    grid_t_0[:, 0] = 1\n",
        "    grid_t_0[-1, :] = 1\n",
        "    grid_t_0[:, -1] = 1\n",
        "\n",
        "    # np.count_nonzero(grid_t_0 == 1) = 296\n",
        "    # Flatten grid into column vector and propagate for each timestep\n",
        "\n",
        "    grid_t_in = np.tile(grid_t_0.reshape(-1, 1), (1, T)) # (N, T)\n",
        "\n",
        "\n",
        "    # Flatten final grid into column vector to be used in training\n",
        "    boundary_positions = grid_t_in.astype(bool).flatten() # (NT,1)\n",
        "\n",
        "    return boundary_positions\n"
      ],
      "metadata": {
        "id": "vT8VLrAQANy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training algorithm"
      ],
      "metadata": {
        "id": "SF4KUI5Y05iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "7ImmJ84mARQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(num_neurons, num_layers, epochs, train_selection):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    num_neurons - int, # of hidden units for each neural network layer\n",
        "    num_layers - int, # of neural network layers\n",
        "    epochs - int, # of training epochs\n",
        "    model - int, whether to use model 1 (Raissi 2019) or model 2 (continuity PDE)\n",
        "    train_selection - float, frac of all data (N*T) to select for training OR\n",
        "                      'BC', select the boundary conditions for training (all timesteps)\n",
        "    \"\"\"\n",
        "    # Load flattened cynlinder wake data\n",
        "    x_in, y_in, t_in, u_in, v_in, p_in, (N, T) = data_loading () # (NT, 1)\n",
        "\n",
        "\n",
        "    # Select training based on a fraction of an existing database or in boundary conditions\n",
        "\n",
        "    if train_selection == 'BC':\n",
        "        idx = boundary_indices(N, T)\n",
        "    else:\n",
        "        samples = int(round(N * T * train_selection))\n",
        "        np.random.seed(0)\n",
        "        idx = np.random.choice(x_in.shape[0], samples, replace=False)\n",
        "\n",
        "    x_train = x_in[idx, :]\n",
        "    y_train = y_in[idx, :]\n",
        "    t_train = t_in[idx, :]\n",
        "    u_train = u_in[idx, :]\n",
        "    v_train = v_in[idx, :]\n",
        "    p_train = p_in[idx, :]\n",
        "\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    PINN_model = NavierStokesModel(num_neurons=num_neurons, num_layers=num_layers)\n",
        "    criterion = NavierStokesLoss()\n",
        "\n",
        "    optimizer = torch.optim.LBFGS(PINN_model.parameters(), line_search_fn='strong_wolfe')\n",
        "\n",
        "    def closure():\n",
        "        \"\"\"Define closure function to use with LBFGS optimizer\"\"\"\n",
        "        optimizer.zero_grad()   # Clear gradients from previous iteration\n",
        "\n",
        "        u_net, v_net, p_net, f, g, h = PINN_model(x_train, y_train, t_train)\n",
        "        loss = criterion(u_train, u_net, v_train, v_net, p_train, p_net, f, g, h)\n",
        "\n",
        "        loss.backward() # Backprogation\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_loop(epochs):\n",
        "        \"\"\"Run full training loop\"\"\"\n",
        "\n",
        "        with open('parameters.csv', 'w', newline='') as csvfile:\n",
        "\n",
        "          writer = csv.writer(csvfile)\n",
        "          writer.writerow(['Epoch', 'Loss','nu','rho'])\n",
        "\n",
        "          for i in tqdm(range(epochs), desc='Training epochs: '):\n",
        "\n",
        "            optimizer.step(closure)\n",
        "            loss = closure().item()\n",
        "            writer.writerow([i + 1, loss, PINN_model.nu.item(), PINN_model.rho.item()])\n",
        "\n",
        "\n",
        "\n",
        "    training_loop(epochs=epochs)\n",
        "\n",
        "    torch.save(PINN_model.state_dict(), 'model_name.pth')\n",
        "    return"
      ],
      "metadata": {
        "id": "yh6rB_P6A9SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the following cell will start the training process. This will produce 2 files: 'model_name.pth', which contains the trained parameters of the model for its evaluation, and 'parameters.csv' which contains the evolution of the predicted parameters along the iterations. The number of neurons, layers, epochs and the training selection fraction can be altered. Initialization of the parameters plays a fundamental role in the performance of the model, this can also be changed at the beginning of the notebook."
      ],
      "metadata": {
        "id": "HphpJb6g1Rsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main(num_neurons=30, num_layers=6, epochs=500, train_selection=0.005)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSO70mc6BBQ2",
        "outputId": "8519da05-8671-4665-a4ff-f555843015a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:   0%|          | 1/200 [00:02<09:54,  2.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.71948,  2.01470]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:   6%|â–Œ         | 11/200 [00:32<09:27,  3.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [-0.00180,  1.35258]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  10%|â–ˆ         | 21/200 [01:00<08:24,  2.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00517,  1.04342]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  16%|â–ˆâ–Œ        | 31/200 [01:31<08:51,  3.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00861,  1.06283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  20%|â–ˆâ–ˆ        | 41/200 [02:04<08:44,  3.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00987,  1.09866]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  26%|â–ˆâ–ˆâ–Œ       | 51/200 [02:35<07:53,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00944,  1.05998]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  30%|â–ˆâ–ˆâ–ˆ       | 61/200 [03:06<07:13,  3.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00894,  1.05253]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/200 [03:38<06:41,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00950,  1.05281]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/200 [04:08<05:53,  2.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00981,  1.05507]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/200 [04:39<05:43,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01018,  1.04511]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/200 [05:10<05:11,  3.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.00996,  1.03691]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/200 [05:43<04:44,  3.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01011,  1.02443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/200 [06:14<04:00,  3.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01004,  1.02060]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/200 [06:46<03:50,  3.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01046,  1.03308]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/200 [07:19<03:18,  3.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01042,  1.03056]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/200 [07:48<02:21,  2.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01048,  1.02161]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/200 [08:19<02:03,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01029,  1.01942]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/200 [08:51<01:30,  3.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01028,  1.02147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/200 [09:24<01:03,  3.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01020,  1.02470]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/200 [09:56<00:27,  3.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðœ†_real = [0.01,  1], ðœ†_PINN = [0.01018,  1.02497]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training epochs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [10:24<00:00,  3.12s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example for the loading and consultation of the parameters' value predicted. In this case the initial values for nu and rho were 0.005 and 0.1 respectively. Every model in this directory was trained on 0.5% of the total data, with 6 layers and 30 neurons per layer."
      ],
      "metadata": {
        "id": "CtpVG99c4NNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_saved_model(num_layers, num_neurons):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "    num_neurons - int, # of hidden units for each neural network layer\n",
        "    num_layers - int, # of neural network layers\n",
        "    \"\"\"\n",
        "    # load saved state_dict\n",
        "    PINN_model = NavierStokesModel(num_neurons=num_neurons, num_layers=num_layers)\n",
        "    PINN_model.load_state_dict(torch.load('/content/PINNs/Models_Data_Figures/NAVIERSTOKES/Inverse/Models/NSInverse_0005_01.pth'))\n",
        "    PINN_model.eval() # Set model to evaluation mode\n",
        "    return PINN_model"
      ],
      "metadata": {
        "id": "bkpHd2L_OEn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelnavierstokes = load_saved_model(num_layers=6, num_neurons=30)"
      ],
      "metadata": {
        "id": "B4pCaJL5ONYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelnavierstokes.rho.detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qCOSZ1C31Q_",
        "outputId": "0f976143-78e4-4e2c-9420-c40c284bf3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.0021018], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelnavierstokes.nu.detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gInD_r9Y32Zn",
        "outputId": "5ac61ea4-29f0-4440-e492-07396d688690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01073081], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}